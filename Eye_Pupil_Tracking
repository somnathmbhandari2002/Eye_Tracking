import cv2
import numpy as np

# Load the face and eye classifiers
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')

# Start capturing video from the webcam
cap = cv2.VideoCapture(1)  # Use 0 for the default webcam

def detect_gaze(eye_roi):
    """
    Detect the position of the eyeball (gaze direction) based on the centroid of the darker area (pupil).
    """
    # Convert the eye region to grayscale and apply a threshold to detect the dark region
    gray_eye = cv2.cvtColor(eye_roi, cv2.COLOR_BGR2GRAY)
    _, threshold = cv2.threshold(gray_eye, 50, 255, cv2.THRESH_BINARY_INV)

    # Find contours in the thresholded eye region
    contours, _ = cv2.findContours(threshold, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)

    if contours:
        # Find the largest contour, which is assumed to be the pupil
        max_contour = max(contours, key=cv2.contourArea)

        # Get the center of the contour (pupil position)
        M = cv2.moments(max_contour)
        if M['m00'] != 0:
            cx = int(M['m10'] / M['m00'])  # X-coordinate of the centroid
            cy = int(M['m01'] / M['m00'])  # Y-coordinate of the centroid

            # Determine gaze direction based on pupil position in the eye ROI
            h, w = gray_eye.shape
            if cx < w * 0.3:
                return "Looking Left"
            elif cx > w * 0.7:
                return "Looking Right"
            elif cy < h * 0.3:
                return "Looking Up"
            elif cy > h * 0.7:
                return "Looking Down"
            else:
                return "Looking Forward"
    return "Unknown"

while True:
    ret, frame = cap.read()
    if not ret:
        print("Failed to capture frame. Exiting...")
        break

    # Convert the frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # Detect faces in the frame
    faces = face_cascade.detectMultiScale(gray, 1.3, 5)

    looking_away = False  # Track if any eye is not looking forward
    eyes_tracked = []  # List to store cropped eye regions

    for (x, y, w, h) in faces:
        # Draw a rectangle around the face
        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)

        # Extract the region of interest for the face
        roi_gray = gray[y:y + h, x:x + w]
        roi_color = frame[y:y + h, x:x + w]

        # Detect eyes in the face region
        eyes = eye_cascade.detectMultiScale(roi_gray)

        for (ex, ey, ew, eh) in eyes:
            # Extract the eye ROI
            eye_roi = roi_color[ey:ey + eh, ex:ex + ew]

            # Add the cropped eye to the list
            eyes_tracked.append(cv2.resize(eye_roi, (100, 50)))  # Resize for uniform display

            # Determine gaze direction
            gaze_direction = detect_gaze(eye_roi)

            # Check if the gaze is not forward
            if gaze_direction != "Looking Forward":
                looking_away = True

            # Draw a rectangle around the eye
            cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (0, 255, 0), 2)

            # Display the gaze direction on the frame
            cv2.putText(frame, gaze_direction, (x + ex, y + ey - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)

    # If the user is not looking forward, display a warning message
    if looking_away:
        cv2.putText(frame, "LOOK FORWARD!", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)

    # Display the main frame
    cv2.imshow('Eye Tracking', frame)

    # Create a blank image for the second screen to display eyes
    if eyes_tracked:
        eye_display = np.zeros((200, 400, 3), dtype=np.uint8)  # Black screen for showing eyes
        for i, eye in enumerate(eyes_tracked):
            x_offset = (i % 4) * 100  # 4 eyes per row
            y_offset = (i // 4) * 50  # Next row after 4 eyes
            eye_display[y_offset:y_offset + 50, x_offset:x_offset + 100] = eye

        # Show the second display with eyes
        cv2.imshow('Tracked Eyes', eye_display)

    # Exit the loop if 'q' is pressed
    if cv2.waitKey(1) == ord('q'):
        break

# Release the video capture and close the window
cap.release()
cv2.destroyAllWindows()
